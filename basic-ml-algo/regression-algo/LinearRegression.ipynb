{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Boston House Price Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_data = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=boston_data.data\n",
    "y=boston_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize Dataset for better and faster converging of gradient descent\n",
    "X = (X - X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split the data for training and testing pupose\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning rate decides how fast is the gradient decent would move in order to converge\n",
    "# Problem with high learning rate is that it might never coverge\n",
    "# Where problem with lower learning rate is that it might take huge amount of time to converge\n",
    "# Its always interseting to play with and find a perfect balance where a learning rate will \n",
    "# converge and also doesnot takes ages to do so\n",
    "learning_rate=0.1 \n",
    "\n",
    "# Iteration count decides how many time you want the gradient descent keeps using training data to reduce error\n",
    "iteration_count=40000\n",
    "\n",
    "# intercept is a constant that determines where the classification line intersects the varriable axes in space \n",
    "# having no intercept will constrain the classification line to pass through orgion of all axes in coordinate \n",
    "# space , and with intercept there will be a good flexiblity for the algorithm to find best classification line\n",
    "intercept = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(yHat,y_train):\n",
    "    temp = yHat-y_train\n",
    "    return np.mean(np.sqrt(temp**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the intecept variable in training and test data if the value is True\n",
    "if intercept:\n",
    "    X_train_intercept = np.ones((X_train.shape[0], 1))\n",
    "    X_test_intercept = np.ones((X_test.shape[0], 1))\n",
    "    X_train = np.concatenate((X_train_intercept, X_train), axis=1)\n",
    "    X_test = np.concatenate((X_test_intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the coefficients\n",
    "theta = np.zeros(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig on Data ; Find coefficients of linear equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the coefficients to zero\n",
    "yHat = np.dot(X_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the direction of change for coefficients\n",
    "gradient = np.dot(X_train.T, (yHat- y_train)) / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re evaluate coefficients by subtracting the multiple of learning rate and gradient\n",
    "theta -= learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Error : 23.015819209\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Error :\",error(yHat,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number :  1000 ; error :  5.35297861294\n",
      "iteration number :  2000 ; error :  5.10549650286\n",
      "iteration number :  3000 ; error :  4.9165991875\n",
      "iteration number :  4000 ; error :  4.75507676995\n",
      "iteration number :  5000 ; error :  4.62804087341\n",
      "iteration number :  6000 ; error :  4.51991534116\n",
      "iteration number :  7000 ; error :  4.42606963717\n",
      "iteration number :  8000 ; error :  4.34409049912\n",
      "iteration number :  9000 ; error :  4.27625361397\n",
      "iteration number :  10000 ; error :  4.22239947922\n",
      "iteration number :  11000 ; error :  4.18472652736\n",
      "iteration number :  12000 ; error :  4.15691982486\n",
      "iteration number :  13000 ; error :  4.1342647397\n",
      "iteration number :  14000 ; error :  4.11784110321\n",
      "iteration number :  15000 ; error :  4.10409585524\n",
      "iteration number :  16000 ; error :  4.0941911164\n",
      "iteration number :  17000 ; error :  4.08572506242\n",
      "iteration number :  18000 ; error :  4.078861767\n",
      "iteration number :  19000 ; error :  4.07257918605\n",
      "iteration number :  20000 ; error :  4.06644471288\n",
      "iteration number :  21000 ; error :  4.06037110506\n",
      "iteration number :  22000 ; error :  4.0542821546\n",
      "iteration number :  23000 ; error :  4.04884088176\n",
      "iteration number :  24000 ; error :  4.04382427007\n",
      "iteration number :  25000 ; error :  4.03896192134\n",
      "iteration number :  26000 ; error :  4.03432643424\n",
      "iteration number :  27000 ; error :  4.02955695288\n",
      "iteration number :  28000 ; error :  4.02483155107\n",
      "iteration number :  29000 ; error :  4.01996693668\n",
      "iteration number :  30000 ; error :  4.01523591873\n",
      "iteration number :  31000 ; error :  4.01047040135\n",
      "iteration number :  32000 ; error :  4.00561935277\n",
      "iteration number :  33000 ; error :  4.0006813025\n",
      "iteration number :  34000 ; error :  3.99582897268\n",
      "iteration number :  35000 ; error :  3.99097526002\n",
      "iteration number :  36000 ; error :  3.98619237201\n",
      "iteration number :  37000 ; error :  3.98154573766\n",
      "iteration number :  38000 ; error :  3.97712618177\n",
      "iteration number :  39000 ; error :  3.97273945959\n"
     ]
    }
   ],
   "source": [
    "# now iterate the above steps for iteration_count-1 times and we have already done it one time\n",
    "for i in range(1,iteration_count):\n",
    "    yHat = np.dot(X_train, theta)\n",
    "    \n",
    "    gradient = np.dot(X_train.T, (yHat- y_train)) / y.size\n",
    "    theta -= learning_rate * gradient\n",
    "    \n",
    "    rmse = error(yHat , y_train)\n",
    "    \n",
    "    if(i%1000==0):\n",
    "        print(\"iteration number : \",i,\"; error : \",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store prediction result\n",
    "prediction=np.dot(X_test, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error : 3.96833501553\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error :\",error(y_train,np.dot(X_train,theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : 3.99579480029\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Error :\",error(prediction,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
