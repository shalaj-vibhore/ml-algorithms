{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cancer Dataset from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data using scikit for training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= cancer.data\n",
    "y= cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decides how fast is the gradient decent would move in order to converge\n",
    "# Problem with high learning rate is that it might never coverge\n",
    "# Where problem with lower learning rate is that it might take huge amount of time to converge\n",
    "# Its always interseting to play with and find a perfect balance where a learning rate will \n",
    "# converge and also doesnot takes ages to do so\n",
    "learning_rate=0.1 \n",
    "\n",
    "# Iteration count decides how many time you want the gradient descent keeps using training data to reduce error\n",
    "iteration_count=2000\n",
    "\n",
    "# intercept is a constant that determines where the classification line intersects the varriable axes in space \n",
    "# having no intercept will constrain the classification line to pass through orgion of all axes in coordinate \n",
    "# space , and with intercept there will be a good flexiblity for the algorithm to find best classification line\n",
    "intercept = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid funtion takes the score calculated by multiplying the actual varriable data with the derived coefficient\n",
    "# and produces a number that fluctuates between 0 and 1 .\n",
    "\n",
    "# Once the final coefficients are derived this is the function that helps in finding the predicted probablity \n",
    "# and round them off to make sure it matches the target class\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this code Log Loss and Error function is used to check on progress of algorithm trying to fit the data\n",
    "def loss(yHat, y, eps=1e-15):\n",
    "    p = np.clip(yHat, eps, 1 - eps)\n",
    "    return np.where(y==1,-np.log(p),-np.log(1 - p)).mean()\n",
    "\n",
    "def error(yHat,y_train):\n",
    "    temp = yHat-y_train\n",
    "    return np.mean(temp**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#theta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the intecept variable in training and test data if the value is True\n",
    "if intercept:\n",
    "    X_train_intercept = np.ones((X_train.shape[0], 1))\n",
    "    X_test_intercept = np.ones((X_test.shape[0], 1))\n",
    "    X_train = np.concatenate((X_train_intercept, X_train), axis=1)\n",
    "    X_test = np.concatenate((X_test_intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the coefficients\n",
    "theta = np.zeros(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on data , i.e finding out coefficients of line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the coefficients to zero\n",
    "z = np.dot(X_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict the classes of observation in data with the zero coefficients\n",
    "yHat= sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate error  and log loss between actual and predicted class\n",
    "avg_error = error(yHat,y_train)\n",
    "\n",
    "loss_val = loss(yHat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the direction of change for coefficients\n",
    "gradient = np.dot(X_train.T, (yHat- y_train)) / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re evaluate coefficients by subtracting the multiple of learning rate and gradient\n",
    "theta -= learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number :  50 ; error :  0.304020100503 ; loss :  21.6084304581\n",
      "iteration number :  100 ; error :  0.283919597989 ; loss :  21.6084304581\n",
      "iteration number :  150 ; error :  0.273869346734 ; loss :  18.2239795038\n",
      "iteration number :  200 ; error :  0.110552763819 ; loss :  6.37061705841\n",
      "iteration number :  250 ; error :  0.108040201005 ; loss :  6.32998190048\n",
      "iteration number :  300 ; error :  0.110552763819 ; loss :  6.32780693259\n",
      "iteration number :  350 ; error :  0.153266331658 ; loss :  10.3545100204\n",
      "iteration number :  400 ; error :  0.110552763819 ; loss :  5.10878837018\n",
      "iteration number :  450 ; error :  0.0979899497487 ; loss :  3.99195202181\n",
      "iteration number :  500 ; error :  0.0954773869347 ; loss :  4.00825926149\n",
      "iteration number :  550 ; error :  0.0954773869347 ; loss :  4.07873186965\n",
      "iteration number :  600 ; error :  0.0929648241206 ; loss :  4.33907440529\n",
      "iteration number :  650 ; error :  0.0954773869347 ; loss :  4.48747101527\n",
      "iteration number :  700 ; error :  0.0979899497487 ; loss :  4.69066619035\n",
      "iteration number :  750 ; error :  0.266331658291 ; loss :  16.4415459457\n",
      "iteration number :  800 ; error :  0.0954773869347 ; loss :  4.20431462224\n",
      "iteration number :  850 ; error :  0.0954773863483 ; loss :  4.32037922174\n",
      "iteration number :  900 ; error :  0.0929648241206 ; loss :  4.07649914233\n",
      "iteration number :  950 ; error :  0.0929648241206 ; loss :  3.90518810561\n",
      "iteration number :  1000 ; error :  0.0929648241206 ; loss :  3.90517422466\n",
      "iteration number :  1050 ; error :  0.0954773869347 ; loss :  3.99195705155\n",
      "iteration number :  1100 ; error :  0.0954773869347 ; loss :  3.99195303347\n",
      "iteration number :  1150 ; error :  0.0954773869347 ; loss :  3.80804375281\n",
      "iteration number :  1200 ; error :  0.0929648241206 ; loss :  3.9051741973\n",
      "iteration number :  1250 ; error :  0.0929648241206 ; loss :  4.07873588772\n",
      "iteration number :  1300 ; error :  0.326633165829 ; loss :  21.0877453868\n",
      "iteration number :  1350 ; error :  0.256281407032 ; loss :  15.5428545856\n",
      "iteration number :  1400 ; error :  0.0954773869347 ; loss :  3.81839536112\n",
      "iteration number :  1450 ; error :  0.0929648241206 ; loss :  3.99195504251\n",
      "iteration number :  1500 ; error :  0.0929648241206 ; loss :  3.81839536126\n",
      "iteration number :  1550 ; error :  0.0929648241206 ; loss :  4.07873789676\n",
      "iteration number :  1600 ; error :  0.0929648241206 ; loss :  3.99195504251\n",
      "iteration number :  1650 ; error :  0.0954773869347 ; loss :  3.70955890067\n",
      "iteration number :  1700 ; error :  0.0954773869347 ; loss :  3.83261639382\n",
      "iteration number :  1750 ; error :  0.0954773869347 ; loss :  3.72502299826\n",
      "iteration number :  1800 ; error :  0.286432160804 ; loss :  18.8314434118\n",
      "iteration number :  1850 ; error :  0.0954773869347 ; loss :  3.81839536112\n",
      "iteration number :  1900 ; error :  0.0929648241206 ; loss :  3.81839536112\n",
      "iteration number :  1950 ; error :  0.0929648241206 ; loss :  3.90802591671\n"
     ]
    }
   ],
   "source": [
    "# now iterate the above steps for iteration_count-1 times and we have already done it one time\n",
    "for i in range(1,iteration_count):\n",
    "    z = np.dot(X_train, theta)\n",
    "    yHat= sigmoid(z)\n",
    "    \n",
    "    avg_error = error(yHat , y_train)\n",
    "    loss_val = loss(yHat, y_train)\n",
    "    \n",
    "    gradient = np.dot(X_train.T, (yHat- y_train)) / y.size\n",
    "    theta -= learning_rate * gradient\n",
    "\n",
    "    if(i%50==0):\n",
    "        print(\"iteration number : \",i,\"; error : \",avg_error,\"; loss : \",loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# store prediction result\n",
    "prediction=y_test==sigmoid(np.dot(X_test, theta)).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.906432748538009"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display accuracy\n",
    "prediction.mean()*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
